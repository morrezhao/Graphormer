{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"metadata":{}},"outputs":[],"source":["import numpy as np\n","\n","def generate_electrons(n_electrons, n_groups):\n","    # 生成每个群的随机三维坐标以及电荷，目前默认电子所以电荷数都是1.\n","    # 使用下面那一行注释内容就可以把电荷数扩充到1-10之间的随机数\n","    positions = np.random.randn(n_groups, n_electrons, 3)\n","    charges = -np.ones((n_groups, n_electrons), dtype=int)  # 每个电子带负电\n","    # charges = -np.random.randint(1, 11, size=(n_groups, n_electrons))  # 每个电子带负电1-10\n","    return positions, charges\n","\n","\n","def calculate_forces_and_potential(positions, charges):\n","    k_e = 1  # 库伦常数，用了原子单位制\n","    n_groups, n_electrons, _ = positions.shape\n","    forces = np.zeros_like(positions)\n","    potential_energy = np.zeros(n_groups, dtype=np.float32)\n","\n","    for group in range(n_groups):\n","        for i in range(n_electrons):\n","            for j in range(i + 1, n_electrons):\n","                r_vec = positions[group, j] - positions[group, i]\n","                r_mag = np.linalg.norm(r_vec)\n","                if r_mag == 0:\n","                    continue  # 避免两个随机出来的坐标恰好完全相同\n","                r_hat = r_vec / r_mag\n","\n","                force_magnitude = k_e * charges[group, i] * charges[group, j] / r_mag ** 2\n","                forces[group, i] -= force_magnitude * r_hat\n","                forces[group, j] += force_magnitude * r_hat\n","                potential_energy[group] += k_e * charges[group, i] * charges[group, j] / r_mag\n","\n","    return forces, potential_energy\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"metadata":{}},"outputs":[],"source":["import torch\n","\n","n_electrons = 5  # 每个电子群中的电子数量\n","n_groups = 10  # 生成10个电子群\n","\n","device = 'cpu'\n","positions, charges = generate_electrons(n_electrons, n_groups)\n","forces, potential_energy = calculate_forces_and_potential(positions, charges)\n","potential_energy = torch.from_numpy(potential_energy)\n","positions_tensor = torch.from_numpy(positions).to(device=device)\n","padding_tensor = torch.zeros((n_groups, 1, 3), device=device)\n","positions_tensor = torch.cat([padding_tensor, positions_tensor], dim=1)\n","positions_tensor.requires_grad = True\n","\n","# 这里用来生成和处理数据"]},{"cell_type":"code","execution_count":3,"metadata":{"metadata":{}},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/mamba/envs/hf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Some weights of GraphormerModel were not initialized from the model checkpoint at clefourrier/graphormer-base-pcqm4mv1 and are newly initialized: ['graph_encoder.emb_layer_norm.bias', 'graph_encoder.emb_layer_norm.weight', 'graph_encoder.graph_attn_bias.edge_dis_encoder.weight', 'graph_encoder.graph_attn_bias.edge_encoder.weight', 'graph_encoder.graph_attn_bias.graph_token_virtual_distance.weight', 'graph_encoder.graph_attn_bias.spatial_pos_encoder.weight', 'graph_encoder.graph_node_feature.atom_encoder.weight', 'graph_encoder.graph_node_feature.graph_token.weight', 'graph_encoder.graph_node_feature.in_degree_encoder.weight', 'graph_encoder.graph_node_feature.out_degree_encoder.weight', 'graph_encoder.layers.0.fc1.bias', 'graph_encoder.layers.0.fc1.weight', 'graph_encoder.layers.0.fc2.bias', 'graph_encoder.layers.0.fc2.weight', 'graph_encoder.layers.0.final_layer_norm.bias', 'graph_encoder.layers.0.final_layer_norm.weight', 'graph_encoder.layers.0.self_attn.k_proj.bias', 'graph_encoder.layers.0.self_attn.k_proj.weight', 'graph_encoder.layers.0.self_attn.out_proj.bias', 'graph_encoder.layers.0.self_attn.out_proj.weight', 'graph_encoder.layers.0.self_attn.q_proj.bias', 'graph_encoder.layers.0.self_attn.q_proj.weight', 'graph_encoder.layers.0.self_attn.v_proj.bias', 'graph_encoder.layers.0.self_attn.v_proj.weight', 'graph_encoder.layers.0.self_attn_layer_norm.bias', 'graph_encoder.layers.0.self_attn_layer_norm.weight', 'graph_encoder.layers.1.fc1.bias', 'graph_encoder.layers.1.fc1.weight', 'graph_encoder.layers.1.fc2.bias', 'graph_encoder.layers.1.fc2.weight', 'graph_encoder.layers.1.final_layer_norm.bias', 'graph_encoder.layers.1.final_layer_norm.weight', 'graph_encoder.layers.1.self_attn.k_proj.bias', 'graph_encoder.layers.1.self_attn.k_proj.weight', 'graph_encoder.layers.1.self_attn.out_proj.bias', 'graph_encoder.layers.1.self_attn.out_proj.weight', 'graph_encoder.layers.1.self_attn.q_proj.bias', 'graph_encoder.layers.1.self_attn.q_proj.weight', 'graph_encoder.layers.1.self_attn.v_proj.bias', 'graph_encoder.layers.1.self_attn.v_proj.weight', 'graph_encoder.layers.1.self_attn_layer_norm.bias', 'graph_encoder.layers.1.self_attn_layer_norm.weight', 'graph_encoder.layers.10.fc1.bias', 'graph_encoder.layers.10.fc1.weight', 'graph_encoder.layers.10.fc2.bias', 'graph_encoder.layers.10.fc2.weight', 'graph_encoder.layers.10.final_layer_norm.bias', 'graph_encoder.layers.10.final_layer_norm.weight', 'graph_encoder.layers.10.self_attn.k_proj.bias', 'graph_encoder.layers.10.self_attn.k_proj.weight', 'graph_encoder.layers.10.self_attn.out_proj.bias', 'graph_encoder.layers.10.self_attn.out_proj.weight', 'graph_encoder.layers.10.self_attn.q_proj.bias', 'graph_encoder.layers.10.self_attn.q_proj.weight', 'graph_encoder.layers.10.self_attn.v_proj.bias', 'graph_encoder.layers.10.self_attn.v_proj.weight', 'graph_encoder.layers.10.self_attn_layer_norm.bias', 'graph_encoder.layers.10.self_attn_layer_norm.weight', 'graph_encoder.layers.11.fc1.bias', 'graph_encoder.layers.11.fc1.weight', 'graph_encoder.layers.11.fc2.bias', 'graph_encoder.layers.11.fc2.weight', 'graph_encoder.layers.11.final_layer_norm.bias', 'graph_encoder.layers.11.final_layer_norm.weight', 'graph_encoder.layers.11.self_attn.k_proj.bias', 'graph_encoder.layers.11.self_attn.k_proj.weight', 'graph_encoder.layers.11.self_attn.out_proj.bias', 'graph_encoder.layers.11.self_attn.out_proj.weight', 'graph_encoder.layers.11.self_attn.q_proj.bias', 'graph_encoder.layers.11.self_attn.q_proj.weight', 'graph_encoder.layers.11.self_attn.v_proj.bias', 'graph_encoder.layers.11.self_attn.v_proj.weight', 'graph_encoder.layers.11.self_attn_layer_norm.bias', 'graph_encoder.layers.11.self_attn_layer_norm.weight', 'graph_encoder.layers.2.fc1.bias', 'graph_encoder.layers.2.fc1.weight', 'graph_encoder.layers.2.fc2.bias', 'graph_encoder.layers.2.fc2.weight', 'graph_encoder.layers.2.final_layer_norm.bias', 'graph_encoder.layers.2.final_layer_norm.weight', 'graph_encoder.layers.2.self_attn.k_proj.bias', 'graph_encoder.layers.2.self_attn.k_proj.weight', 'graph_encoder.layers.2.self_attn.out_proj.bias', 'graph_encoder.layers.2.self_attn.out_proj.weight', 'graph_encoder.layers.2.self_attn.q_proj.bias', 'graph_encoder.layers.2.self_attn.q_proj.weight', 'graph_encoder.layers.2.self_attn.v_proj.bias', 'graph_encoder.layers.2.self_attn.v_proj.weight', 'graph_encoder.layers.2.self_attn_layer_norm.bias', 'graph_encoder.layers.2.self_attn_layer_norm.weight', 'graph_encoder.layers.3.fc1.bias', 'graph_encoder.layers.3.fc1.weight', 'graph_encoder.layers.3.fc2.bias', 'graph_encoder.layers.3.fc2.weight', 'graph_encoder.layers.3.final_layer_norm.bias', 'graph_encoder.layers.3.final_layer_norm.weight', 'graph_encoder.layers.3.self_attn.k_proj.bias', 'graph_encoder.layers.3.self_attn.k_proj.weight', 'graph_encoder.layers.3.self_attn.out_proj.bias', 'graph_encoder.layers.3.self_attn.out_proj.weight', 'graph_encoder.layers.3.self_attn.q_proj.bias', 'graph_encoder.layers.3.self_attn.q_proj.weight', 'graph_encoder.layers.3.self_attn.v_proj.bias', 'graph_encoder.layers.3.self_attn.v_proj.weight', 'graph_encoder.layers.3.self_attn_layer_norm.bias', 'graph_encoder.layers.3.self_attn_layer_norm.weight', 'graph_encoder.layers.4.fc1.bias', 'graph_encoder.layers.4.fc1.weight', 'graph_encoder.layers.4.fc2.bias', 'graph_encoder.layers.4.fc2.weight', 'graph_encoder.layers.4.final_layer_norm.bias', 'graph_encoder.layers.4.final_layer_norm.weight', 'graph_encoder.layers.4.self_attn.k_proj.bias', 'graph_encoder.layers.4.self_attn.k_proj.weight', 'graph_encoder.layers.4.self_attn.out_proj.bias', 'graph_encoder.layers.4.self_attn.out_proj.weight', 'graph_encoder.layers.4.self_attn.q_proj.bias', 'graph_encoder.layers.4.self_attn.q_proj.weight', 'graph_encoder.layers.4.self_attn.v_proj.bias', 'graph_encoder.layers.4.self_attn.v_proj.weight', 'graph_encoder.layers.4.self_attn_layer_norm.bias', 'graph_encoder.layers.4.self_attn_layer_norm.weight', 'graph_encoder.layers.5.fc1.bias', 'graph_encoder.layers.5.fc1.weight', 'graph_encoder.layers.5.fc2.bias', 'graph_encoder.layers.5.fc2.weight', 'graph_encoder.layers.5.final_layer_norm.bias', 'graph_encoder.layers.5.final_layer_norm.weight', 'graph_encoder.layers.5.self_attn.k_proj.bias', 'graph_encoder.layers.5.self_attn.k_proj.weight', 'graph_encoder.layers.5.self_attn.out_proj.bias', 'graph_encoder.layers.5.self_attn.out_proj.weight', 'graph_encoder.layers.5.self_attn.q_proj.bias', 'graph_encoder.layers.5.self_attn.q_proj.weight', 'graph_encoder.layers.5.self_attn.v_proj.bias', 'graph_encoder.layers.5.self_attn.v_proj.weight', 'graph_encoder.layers.5.self_attn_layer_norm.bias', 'graph_encoder.layers.5.self_attn_layer_norm.weight', 'graph_encoder.layers.6.fc1.bias', 'graph_encoder.layers.6.fc1.weight', 'graph_encoder.layers.6.fc2.bias', 'graph_encoder.layers.6.fc2.weight', 'graph_encoder.layers.6.final_layer_norm.bias', 'graph_encoder.layers.6.final_layer_norm.weight', 'graph_encoder.layers.6.self_attn.k_proj.bias', 'graph_encoder.layers.6.self_attn.k_proj.weight', 'graph_encoder.layers.6.self_attn.out_proj.bias', 'graph_encoder.layers.6.self_attn.out_proj.weight', 'graph_encoder.layers.6.self_attn.q_proj.bias', 'graph_encoder.layers.6.self_attn.q_proj.weight', 'graph_encoder.layers.6.self_attn.v_proj.bias', 'graph_encoder.layers.6.self_attn.v_proj.weight', 'graph_encoder.layers.6.self_attn_layer_norm.bias', 'graph_encoder.layers.6.self_attn_layer_norm.weight', 'graph_encoder.layers.7.fc1.bias', 'graph_encoder.layers.7.fc1.weight', 'graph_encoder.layers.7.fc2.bias', 'graph_encoder.layers.7.fc2.weight', 'graph_encoder.layers.7.final_layer_norm.bias', 'graph_encoder.layers.7.final_layer_norm.weight', 'graph_encoder.layers.7.self_attn.k_proj.bias', 'graph_encoder.layers.7.self_attn.k_proj.weight', 'graph_encoder.layers.7.self_attn.out_proj.bias', 'graph_encoder.layers.7.self_attn.out_proj.weight', 'graph_encoder.layers.7.self_attn.q_proj.bias', 'graph_encoder.layers.7.self_attn.q_proj.weight', 'graph_encoder.layers.7.self_attn.v_proj.bias', 'graph_encoder.layers.7.self_attn.v_proj.weight', 'graph_encoder.layers.7.self_attn_layer_norm.bias', 'graph_encoder.layers.7.self_attn_layer_norm.weight', 'graph_encoder.layers.8.fc1.bias', 'graph_encoder.layers.8.fc1.weight', 'graph_encoder.layers.8.fc2.bias', 'graph_encoder.layers.8.fc2.weight', 'graph_encoder.layers.8.final_layer_norm.bias', 'graph_encoder.layers.8.final_layer_norm.weight', 'graph_encoder.layers.8.self_attn.k_proj.bias', 'graph_encoder.layers.8.self_attn.k_proj.weight', 'graph_encoder.layers.8.self_attn.out_proj.bias', 'graph_encoder.layers.8.self_attn.out_proj.weight', 'graph_encoder.layers.8.self_attn.q_proj.bias', 'graph_encoder.layers.8.self_attn.q_proj.weight', 'graph_encoder.layers.8.self_attn.v_proj.bias', 'graph_encoder.layers.8.self_attn.v_proj.weight', 'graph_encoder.layers.8.self_attn_layer_norm.bias', 'graph_encoder.layers.8.self_attn_layer_norm.weight', 'graph_encoder.layers.9.fc1.bias', 'graph_encoder.layers.9.fc1.weight', 'graph_encoder.layers.9.fc2.bias', 'graph_encoder.layers.9.fc2.weight', 'graph_encoder.layers.9.final_layer_norm.bias', 'graph_encoder.layers.9.final_layer_norm.weight', 'graph_encoder.layers.9.self_attn.k_proj.bias', 'graph_encoder.layers.9.self_attn.k_proj.weight', 'graph_encoder.layers.9.self_attn.out_proj.bias', 'graph_encoder.layers.9.self_attn.out_proj.weight', 'graph_encoder.layers.9.self_attn.q_proj.bias', 'graph_encoder.layers.9.self_attn.q_proj.weight', 'graph_encoder.layers.9.self_attn.v_proj.bias', 'graph_encoder.layers.9.self_attn.v_proj.weight', 'graph_encoder.layers.9.self_attn_layer_norm.bias', 'graph_encoder.layers.9.self_attn_layer_norm.weight', 'layer_norm.bias', 'layer_norm.weight', 'lm_head_transform_weight.bias', 'lm_head_transform_weight.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# 使用预定义的模型，为了简化，multi_hop_max_dist取0\n","\n","from transformers import GraphormerModel, AdamW, GraphormerConfig\n","import os\n","os.environ['HTTP_PROXY'] = 'http://ga.dp.tech:8118'\n","os.environ['HTTPS_PROXY'] = 'http://ga.dp.tech:8118'\n","\n","model_name = \"clefourrier/graphormer-base-pcqm4mv1\"\n","\n","backbone_model = GraphormerModel.from_pretrained(model_name)\n","config = GraphormerConfig.from_pretrained(model_name)\n","config.multi_hop_max_dist = 0"]},{"cell_type":"code","execution_count":12,"metadata":{"metadata":{}},"outputs":[],"source":["from transformers import GraphormerModel\n","import torch\n","import torch.nn as nn\n","\n","# 定义新的注意力头，用于预测势能\n","\n","class CustomGraphormerModel(nn.Module):\n","    def __init__(self, graphormer_model):\n","        super().__init__()\n","        self.graphormer = graphormer_model\n","        # self.force_head = nn.Linear(768, 3)  # 输出力的线性层，假设每个原子有三个力分量\n","        self.energy_head = nn.Linear(768, 1)  # 输出势能的线性层\n","    \n","    def convert_to_inputs(self, positions):\n","        n_groups, n_electrons, _ = positions.shape\n","        # 这是因为graphormer引入了一个虚节点，position[:, 0]用了一个占位符[0, 0, 0]，是没有任何意义的，所以force[:,1:]才有意义。真实的电子数是positions.shape[1]-1\n","        n_electrons -= 1\n","        # device = 'cuda' if torch.cuda.is_available() else 'cpu' （服务器内存好像不太够，先用cpu了）\n","        device = 'cpu'\n","\n","        # 初始化Tensor\n","        # input_nodes = torch.ones((n_groups, n_electrons, 1), device=device, dtype=int)  # [num_graphs, num_nodes, 1]\n","        input_nodes = -torch.from_numpy(charges).unsqueeze(-1).to(device=device)\n","        input_edges = torch.zeros((n_groups, n_electrons, n_electrons, 1, 1), device=device, dtype=int) # [num_graphs, num_nodes, num_nodes, max_hop, num_edge_features]\n","        # attn_bias = torch.zeros((n_groups, n_electrons + 1, n_electrons + 1), device=device)  \n","        in_degree = torch.ones((n_groups, n_electrons), device=device, dtype=int) * (n_electrons - 1)\n","        out_degree = torch.ones((n_groups, n_electrons), device=device, dtype=int) * (n_electrons - 1)\n","        spatial_pos = torch.zeros((n_groups, n_electrons, n_electrons), device=device, dtype=int)  \n","        # spatial_pos = torch.arange(n_electrons).unsqueeze(0).repeat(n_groups, 1).to(device)\n","        attn_edge_type = torch.zeros((n_groups, n_electrons, n_electrons), device=device, dtype=int)\n","\n","        # 计算边特征为距离的倒数\n","        for group in range(n_groups):\n","            for i in range(n_electrons):\n","                for j in range(n_electrons):\n","                    if i != j:\n","                        # distance = torch.norm(positions[group, i] - positions[group, j])\n","                        # attn_bias[group, i + 1, j + 1] = distance\n","                        input_edges[group, i, j, 0, 0] = 1\n","                        spatial_pos[group, i, j] = 1\n","        expanded_positions_i = positions.unsqueeze(2).repeat(1, 1, n_electrons + 1, 1)\n","        expanded_positions_j = positions.unsqueeze(1).repeat(1, n_electrons + 1, 1, 1)\n","        # print(expanded_positions_i.shape, expanded_positions_j.shape)\n","\n","    # 计算所有配对的欧几里得距离\n","        attn_bias = torch.norm(expanded_positions_i - expanded_positions_j, dim=3)\n","        # print(\"positions_grad:\", torch.autograd.grad(attn_bias[0][1][2], positions, create_graph=True)[0])\n","\n","        graphormer_inputs = {\n","            'input_nodes': input_nodes,\n","            'input_edges': input_edges,\n","            'attn_bias': attn_bias,\n","            'in_degree': in_degree,\n","            'out_degree': out_degree,\n","            'spatial_pos': spatial_pos,\n","            'attn_edge_type': attn_edge_type\n","        }\n","        \n","        return graphormer_inputs\n","\n","    def forward(self, positions):\n","        inputs = self.convert_to_inputs(positions=positions)\n","        outputs = self.graphormer(**inputs)\n","        pooled_output = outputs.last_hidden_state.mean(dim=1)  # 从Graphormer输出中提取池化表示\n","        # forces = self.force_head(pooled_output)  # 预测力  \n","        energy = self.energy_head(pooled_output)  # 预测势能\n","        # forces_1 = torch.zeros_like(positions)\n","        # print(torch.autograd.grad(energy[0], self.positions, create_graph=True, allow_unused=True))\n","        grad_outputs = torch.ones_like(energy)\n","        forces = -torch.autograd.grad(energy, positions, create_graph=True, grad_outputs=grad_outputs)[0]     \n","        # 用两种方法算了力，可以确定结果是对的。     \n","        # for i in range(energy.shape[0]): \n","        #     # print(torch.autograd.grad(energy[i], self.positions, create_graph=True)[0][i])\n","        #     forces_1[i] = -torch.autograd.grad(energy[i], positions, retain_graph=True)[0][i] \n","        # assert torch.allclose(forces, forces_1)\n","        return forces, energy\n","    \n","\n","\n","# positions_tensor = torch.from_numpy(positions).to(device=device)\n","# padding_tensor = torch.zeros((n_groups, 1, 3), device=device)\n","# positions_tensor = torch.cat([padding_tensor, positions_tensor], dim=1)\n","# positions_tensor.requires_grad = True\n","model = CustomGraphormerModel(backbone_model).to(device=device)"]},{"cell_type":"code","execution_count":5,"metadata":{"metadata":{}},"outputs":[],"source":["### use for debug\n","\n","# positions_tensor.requires_grad = True\n","\n","# expanded_positions_i = positions_tensor.unsqueeze(2).repeat(1, 1, n_electrons + 1, 1)\n","# expanded_positions_j = positions_tensor.unsqueeze(1).repeat(1, n_electrons + 1, 1, 1)\n","# attn_bias = torch.norm(expanded_positions_i - expanded_positions_j, dim=3)\n","# # attn_bias[0][1][2]\n","# print(\"positions_grad:\", torch.autograd.grad(attn_bias[0][1][2], positions_tensor, create_graph=True)[0])\n","pred_forces, pred_energy = model(positions_tensor)"]},{"cell_type":"code","execution_count":57,"metadata":{"metadata":{}},"outputs":[{"data":{"text/plain":["tensor([[-0.2579],\n","        [-0.2610],\n","        [-0.2533],\n","        [-0.2615],\n","        [-0.2614],\n","        [-0.2595],\n","        [-0.2612],\n","        [-0.2600],\n","        [-0.2516],\n","        [-0.2615]], grad_fn=<AddmmBackward0>)"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["pred_energy\n","# potential_energy = torch.from_numpy(potential_energy)\n","# loss = loss_fn(pred_energy[0], potential_energy[0])\n","# loss.backward"]},{"cell_type":"code","execution_count":15,"metadata":{"metadata":{}},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/mamba/envs/hf/lib/python3.8/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["-------------------- Epoch 0 --------------------\n","loss tensor(4.1600, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[-0.19959789 -0.47471131  0.05185132]\n"," [ 0.19861299  0.539334    0.12678394]\n"," [-0.42483825 -0.42106038  0.34537007]\n"," [ 0.59346512 -0.18685517 -0.33637865]\n"," [-0.39256933  0.44777039 -0.15269959]]\n","true force: [[-0.02803536 -0.42691148 -0.0668121 ]\n"," [ 0.493071    1.09662785  0.24224822]\n"," [-0.69297084 -0.86143337  0.28443448]\n"," [ 0.58081515 -0.22222195 -0.31482727]\n"," [-0.35287995  0.41393895 -0.14504333]]\n","energy_diff: tensor([0.5231, 0.6869, 0.3414, 0.5071, 0.9996, 0.8003, 0.6164, 0.0000, 0.6466,\n","        0.6283], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 100 --------------------\n","loss tensor(11.3438, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[-0.13019686 -0.50601745  0.26324169]\n"," [ 0.06803811  0.07110426 -0.93015279]\n"," [-0.22429854  0.83636849  0.09865923]\n"," [-0.08685573 -0.37571383  1.35142701]\n"," [ 0.1183872  -0.1049543   0.00878401]]\n","true force: [[-1.74263736 -1.40547837 -0.92307775]\n"," [ 0.06909818 -0.03085788 -0.74100556]\n"," [-0.15607855  0.64493285 -0.0130167 ]\n"," [ 0.55098393 -0.26329373  2.81606789]\n"," [ 1.27863379  1.05469714 -1.13896789]]\n","energy_diff: tensor([1.1696, 1.1361, 1.9592, 1.5722, 2.2010, 0.0448, 2.4794, 0.0000, 2.1159,\n","        1.1640], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 200 --------------------\n","loss tensor(6.8969, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[-0.20276553 -0.1865666  -0.18380988]\n"," [-0.38986543 -0.18562237  0.24512438]\n"," [ 0.12437212 -0.54272771  0.74159827]\n"," [-0.87909714  0.29189255 -1.07610985]\n"," [ 0.96720609  0.66253165  0.07125084]]\n","true force: [[ 3.24403599 -3.403788   -7.70996714]\n"," [-3.3733925   3.09840903  7.8324897 ]\n"," [ 0.16420401 -0.61177474  0.73806837]\n"," [-0.87035243  0.33188941 -0.78152319]\n"," [ 0.83550494  0.5852643  -0.07906773]]\n","energy_diff: tensor([0.0000, 1.9228, 2.0391, 1.7072, 2.1338, 1.7571, 2.4390, 2.0641, 1.9525,\n","        1.9925], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 300 --------------------\n","loss tensor(12.3826, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 0.04776742 -0.08591746  0.52831069]\n"," [-0.24670419  0.78049976 -0.55890726]\n"," [-0.54827763 -0.09448277  0.22128618]\n"," [ 0.54852191 -0.01756951  0.18733378]\n"," [ 0.03546118 -1.12550457 -0.86384329]]\n","true force: [[ 0.74161017 -0.64027361  1.16519555]\n"," [ 0.08252743  1.24330558 -1.10106947]\n"," [-1.42362308  0.05540288  0.24916634]\n"," [ 0.48634061  0.13727202  0.14796107]\n"," [ 0.11314487 -0.79570687 -0.46125349]]\n","energy_diff: tensor([3.7871, 4.1699, 2.6681, 4.0178, 4.1954, 0.0000, 4.0602, 3.9662, 3.8562,\n","        4.1312], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 400 --------------------\n","loss tensor(9.3875, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[-0.10232255 -0.9313341   0.43899121]\n"," [-0.44411759  0.35402074  0.90991537]\n"," [ 0.00801753 -0.17870225 -0.10732145]\n"," [-0.37246255  0.49907763 -0.86451988]\n"," [ 0.9321255   0.01947241 -0.0300744 ]]\n","true force: [[-0.22291177 -1.19993227  0.33948872]\n"," [-0.9568807   0.75835568  1.0283269 ]\n"," [-0.03731106 -0.23542691 -0.25341518]\n"," [-0.30037985  0.36660208 -0.67722382]\n"," [ 1.51748338  0.31040142 -0.43717662]]\n","energy_diff: tensor([1.8082, 1.8402, 2.1026, 0.7830, 1.8506, 1.8909, 1.8459, 0.0000, 1.0455,\n","        1.9561], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 500 --------------------\n","loss tensor(2.1284, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 0.15923837  0.00384598  0.62146789]\n"," [ 0.85066191 -0.92813844 -0.13292066]\n"," [-0.58508535 -1.05509952 -1.31888231]\n"," [-0.32251201  0.60127713  0.89043597]\n"," [-0.42813618  1.71910707  0.03820023]]\n","true force: [[ 0.27311198 -0.05943029  0.64260606]\n"," [ 0.76164317 -0.68389771 -0.09283148]\n"," [-0.4891372  -1.69052942 -1.54472307]\n"," [-0.29043175  0.29475787  1.0915693 ]\n"," [-0.25518621  2.13909955 -0.0966208 ]]\n","energy_diff: tensor([0.0000, 0.1559, 0.4721, 0.0833, 0.1566, 0.4363, 0.3405, 0.0737, 0.1976,\n","        0.5255], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 600 --------------------\n","loss tensor(12.3126, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[-1.17156047  0.80563034 -1.83315285]\n"," [-0.38357045  1.09061349  0.82899883]\n"," [-0.39774319 -0.68519775  2.21973016]\n"," [ 0.38159599 -1.64928327 -0.83133444]\n"," [ 1.0459787   0.34981031 -0.52627898]]\n","true force: [[ -0.99326203   0.18654077  -1.23259937]\n"," [-14.97182067   6.12322531  12.80807953]\n"," [ -0.47038499  -1.00792598   1.40552327]\n"," [  0.35365062  -1.28095711  -0.54485471]\n"," [ 16.08181707  -4.02088299 -12.43614872]]\n","energy_diff: tensor([0.0000, 2.6127, 1.8927, 1.7145, 2.7960, 2.0188, 2.5022, 0.9485, 2.8975,\n","        2.5543], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 700 --------------------\n","loss tensor(6.0199, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 1.39134402  0.50188905 -0.5899165 ]\n"," [ 0.68063441  0.1659088   0.43782627]\n"," [ 0.30369708 -0.04118279 -0.15791661]\n"," [ 0.17090546 -0.32088498 -0.16123539]\n"," [ 0.05011395 -0.14803037 -0.12152054]]\n","true force: [[ 0.3709827   0.22239695 -0.35862213]\n"," [ 0.254612    0.12233784  0.47623196]\n"," [-0.18131873  0.56193645  0.35184887]\n"," [-0.00146413 -0.5134525  -0.18631117]\n"," [-0.44281185 -0.39321874 -0.28314753]]\n","energy_diff: tensor([1.0770, 0.7697, 0.9907, 0.6685, 0.8192, 0.6917, 1.0644, 0.6174, 0.0000,\n","        0.9226], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 800 --------------------\n","loss tensor(6.5781, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[-0.46886606  0.63437417  0.5525635 ]\n"," [-0.03242242 -0.15234543 -0.01468393]\n"," [ 0.07340355 -0.14408768 -0.02794261]\n"," [-0.01425405  0.26388171 -0.01466997]\n"," [-0.05071637 -0.05446772  0.05911109]]\n","true force: [[-0.1246451   0.07248794  0.1288597 ]\n"," [-0.58686167 -0.52889943 -0.1322357 ]\n"," [ 0.69340648 -0.3678378  -0.16960711]\n"," [ 0.15205355  0.61528547 -0.36289952]\n"," [-0.13395326  0.20896382  0.53588264]]\n","energy_diff: tensor([1.2303, 1.6662, 2.0876, 0.7836, 1.1032, 1.4436, 1.2831, 1.7886, 1.5890,\n","        0.0000], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 900 --------------------\n","loss tensor(6.6745, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 0.0692691   0.03482946  0.87002127]\n"," [ 0.50733761 -0.27920167 -0.27821955]\n"," [-0.17235431 -0.29018648 -0.60248541]\n"," [-0.38961424  0.78186695 -0.03437277]\n"," [-0.36211977 -0.27532299  0.4054156 ]]\n","true force: [[-0.04834395  0.08053268  1.00799914]\n"," [ 2.56235631 -0.60541422 -1.30851763]\n"," [-0.16287623 -0.19730508 -0.44577099]\n"," [-0.29215498  0.69210018 -0.13187657]\n"," [-2.05898114  0.03008644  0.87816605]]\n","energy_diff: tensor([0.8781, 0.0000, 1.4230, 1.3511, 0.9562, 1.3491, 2.0125, 1.5116, 1.2039,\n","        1.0563], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 1000 --------------------\n","loss tensor(6.1336, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 0.1150411  -0.07739793  0.31079754]\n"," [-0.22295018 -0.81601751  0.64851752]\n"," [-0.02206632  0.11282354 -0.13422537]\n"," [-0.07042594 -0.19942734  0.00584384]\n"," [-0.3135629   0.13373435 -0.16808476]]\n","true force: [[ 0.07598571  0.08469907  0.47708966]\n"," [-0.20676666 -0.38418159  0.67213815]\n"," [ 0.41567449  0.14616591 -0.43288131]\n"," [-0.03258178 -0.1465761  -0.4552031 ]\n"," [-0.25231176  0.29989271 -0.26114339]]\n","energy_diff: tensor([1.3026, 1.1603, 0.0000, 0.3317, 1.6294, 1.0444, 0.8353, 0.2823, 1.1390,\n","        1.1598], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 1100 --------------------\n","loss tensor(2.2904, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[-0.22544582  0.13091235  0.10220714]\n"," [ 0.30310446  0.41384236  0.71593829]\n"," [-0.38738824 -0.47099696  0.04787473]\n"," [ 0.18055212 -0.33949284 -0.20703486]\n"," [ 0.24793298  0.2456232  -0.48547518]]\n","true force: [[-0.27008663  0.24258608  0.217947  ]\n"," [ 0.19677757  0.43637037  0.823771  ]\n"," [-0.48207448 -0.51755295 -0.04879332]\n"," [ 0.29629773 -0.44822139 -0.28347076]\n"," [ 0.25908582  0.28681789 -0.70945393]]\n","energy_diff: tensor([0.4283, 0.2326, 0.3970, 0.2484, 0.3419, 0.4692, 0.0132, 0.7923, 0.0000,\n","        0.4334], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 1200 --------------------\n","loss tensor(30.7063, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 0.29281517 -0.17155248 -0.00478898]\n"," [ 0.035367    0.41266137 -0.09666912]\n"," [ 0.85275865 -0.88697253 -0.23804264]\n"," [-0.2089489  -0.27361708  0.33431918]\n"," [-0.21463898  0.04858155 -0.25322975]]\n","true force: [[ 0.31913239 -0.01556226 -0.23891576]\n"," [ 0.0683827   0.62358054  0.15778008]\n"," [ 0.61479582 -0.64733915 -0.07814944]\n"," [-0.30440577 -0.19297015  0.55502585]\n"," [-0.69790515  0.23229102 -0.39574073]]\n","energy_diff: tensor([7.7566, 7.5487, 7.8553, 7.7017, 0.0000, 7.5611, 7.9040, 7.4245, 7.7684,\n","        7.6106], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 1300 --------------------\n","loss tensor(11.1407, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 0.13526283  0.1231477   0.03321687]\n"," [ 0.0330148   0.29948788 -0.47268395]\n"," [ 0.29503729  0.5292489  -0.12305809]\n"," [ 0.08451366 -0.03079586  0.07013895]\n"," [ 1.44130595  0.47070801 -0.16823813]]\n","true force: [[-0.40189058  0.0029449  -0.06951186]\n"," [-0.10838242  0.33546136 -0.40023083]\n"," [-0.04821788 -0.30848999  0.2363169 ]\n"," [ 0.21042727 -0.14426897  0.17069232]\n"," [ 0.3480636   0.11435269  0.06273347]]\n","energy_diff: tensor([2.3317, 2.1410, 2.3323, 1.9897, 0.0000, 0.4013, 1.9368, 0.5976, 2.1056,\n","        1.8103], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 1400 --------------------\n","loss tensor(3.3356, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[-0.19362389  0.27877805 -0.3467098 ]\n"," [ 0.35142396  0.5542886   0.26906753]\n"," [-0.6775297  -1.08717254  0.2167965 ]\n"," [ 0.03670356 -0.20674476 -0.60452758]\n"," [ 0.15453592 -0.23478792  0.16471806]]\n","true force: [[-0.28809517  0.31632648 -0.25139156]\n"," [ 0.31028676  0.5531391   0.32220917]\n"," [-0.68550621 -0.64775364  0.32698028]\n"," [ 0.53737526 -0.00460383 -1.14198304]\n"," [ 0.12593935 -0.21710811  0.74418516]]\n","energy_diff: tensor([0.6232, 0.5889, 0.6962, 0.0000, 0.7072, 0.6739, 0.7950, 0.5315, 0.7072,\n","        0.2823], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 1500 --------------------\n","loss tensor(8.5260, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 0.061385    0.09976735  0.15323669]\n"," [ 1.80848054 -0.72461946  0.34125237]\n"," [-1.5241007  -1.39266945 -1.51179949]\n"," [-0.76968379  1.31981691  0.19403527]\n"," [-0.24614999  0.32395168  0.18822578]]\n","true force: [[ 0.13540156  0.27361734  0.26025659]\n"," [ 1.74603156 -0.45804538  0.39854189]\n"," [-1.77310919 -1.54942286 -1.60126752]\n"," [-0.46488636  3.63688095  1.19713294]\n"," [ 0.35656244 -1.90303004 -0.2546639 ]]\n","energy_diff: tensor([0.8657, 1.1100, 1.8277, 0.5037, 2.1456, 0.0000, 1.2710, 1.7153, 2.0194,\n","        1.6662], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 1600 --------------------\n","loss tensor(10.0611, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 0.77683014  1.35554622 -0.47562631]\n"," [-0.06530215 -0.54152824  0.59150693]\n"," [ 0.19009044 -2.01607668 -0.3616045 ]\n"," [-0.49582854  0.52197147  0.45463014]\n"," [-0.56249831  1.06972272 -0.08759138]]\n","true force: [[ 1.02932349  1.04272282 -0.55048652]\n"," [-0.11413281 -0.51553574  0.55090065]\n"," [ 0.0149713  -2.03597329 -0.37386153]\n"," [-0.35895267  0.35504955  0.44075296]\n"," [-0.57120931  1.15373665 -0.06730557]]\n","energy_diff: tensor([1.5303, 1.3577, 0.0000, 1.4719, 1.5387, 0.0622, 1.6912, 1.7036, 1.4533,\n","        1.3204], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 1700 --------------------\n","loss tensor(13.2115, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 0.50703781  0.16875303  0.05317304]\n"," [ 0.3976533  -0.63797017  0.1331052 ]\n"," [-1.12283939  0.05003978  0.331749  ]\n"," [-0.35856076  0.17656426 -0.85750708]\n"," [-0.26159133  0.25499119  0.34321282]]\n","true force: [[ 1.47735744  0.32336403  0.00580425]\n"," [ 0.45247511 -0.64343804 -0.01957879]\n"," [-0.7126698   0.05770057  0.38979105]\n"," [-0.16538597 -0.02146359 -0.93067965]\n"," [-1.05177678  0.28383703  0.55466314]]\n","energy_diff: tensor([2.9451, 3.0954, 0.0000, 2.6609, 2.1289, 1.8969, 2.6803, 3.2472, 2.8764,\n","        3.2773], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 1800 --------------------\n","loss tensor(4.6647, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[-0.06607746 -1.66285962 -1.12191559]\n"," [ 0.76492273  0.00252098  0.24342453]\n"," [-0.40470509  1.19005597  0.87354662]\n"," [ 0.52778468  0.22507235  0.78931551]\n"," [-0.82872608  0.67475119 -0.81970626]]\n","true force: [[ 0.06387976 -1.4355025  -0.85578665]\n"," [ 1.16087642 -0.35972551  0.16455288]\n"," [-0.84515762  1.36145118  1.10215297]\n"," [ 0.6666351  -0.01532173  0.74333648]\n"," [-1.04623367  0.44909856 -1.15425568]]\n","energy_diff: tensor([0.3510, 0.6486, 0.4617, 0.7526, 0.5145, 0.2710, 0.4205, 0.1392, 0.4867,\n","        0.0000], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 1900 --------------------\n","loss tensor(5.8022, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[-0.19406432 -0.14952772  0.33525887]\n"," [-0.62398623  0.34708493 -0.12047825]\n"," [ 0.3980234  -0.53257986 -0.00395059]\n"," [-0.06129118  0.44132809 -0.1618715 ]\n"," [ 0.30590187 -0.09756205 -0.02709948]]\n","true force: [[-0.1011799  -0.14039492  0.31353621]\n"," [-0.45321644  0.21496076 -0.15175749]\n"," [ 0.22912154 -0.6310473   0.01059149]\n"," [-0.49736841  0.86873432 -0.07033726]\n"," [ 0.8226432  -0.31225286 -0.10203296]]\n","energy_diff: tensor([1.7372, 1.8826, 1.8097, 1.8613, 0.0000, 1.7696, 1.7871, 1.8000, 1.6785,\n","        1.6924], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 2000 --------------------\n","loss tensor(9.1428, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[-0.60928936 -0.0414187  -0.25616301]\n"," [ 0.16646545  0.13169143  0.11642519]\n"," [ 0.24301234 -0.55769181  0.01690709]\n"," [ 0.12935867  0.05148903 -0.11284864]\n"," [ 0.54046941  0.39998619  0.5202355 ]]\n","true force: [[-0.63375906 -0.04292382 -0.08036242]\n"," [ 0.36829251  0.25413448  0.50868222]\n"," [ 0.14065626 -0.64588721  0.03218736]\n"," [-0.05691353  0.12188735 -0.77241031]\n"," [ 0.18172382  0.31278919  0.31190315]]\n","energy_diff: tensor([2.4176, 2.3994, 2.2600, 0.0000, 2.9184, 2.3267, 2.2650, 2.7124, 2.1191,\n","        1.6355], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 2100 --------------------\n","loss tensor(4.2890, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 0.01741846  0.03142003  0.0249587 ]\n"," [-0.33603345 -0.03601588 -0.09887815]\n"," [ 0.10964178  0.05055855  0.05652169]\n"," [-0.74245184  1.63379594 -0.05746666]\n"," [-0.15762228  0.60105072  0.23661425]]\n","true force: [[ 1.5512819  -0.49420106  1.34127215]\n"," [-2.38527243 -0.02983885 -1.5560141 ]\n"," [ 0.82301175  0.22452437  0.37850134]\n"," [-0.11321562  0.55804098 -0.0562174 ]\n"," [ 0.1241944  -0.25852544 -0.107542  ]]\n","energy_diff: tensor([0.0000, 0.4825, 0.9984, 0.3319, 0.7218, 0.8417, 0.7446, 1.0569, 0.6875,\n","        1.0881], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 2200 --------------------\n","loss tensor(3.0350, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 0.23470815 -0.27564214  0.08832844]\n"," [-0.40068393  0.03719727 -0.45214945]\n"," [-0.3856481   0.39958301 -0.18841467]\n"," [ 0.00309152 -0.03491783  0.60003829]\n"," [ 0.22176097 -0.02343251 -0.09180961]]\n","true force: [[ 0.25959901 -0.32765677  0.13501299]\n"," [-0.19036224 -0.05524588 -0.35236452]\n"," [-0.48833388  0.66854219 -0.46247722]\n"," [ 0.09387173 -0.31306542  0.9399345 ]\n"," [ 0.32522538  0.02742587 -0.26010575]]\n","energy_diff: tensor([0.1491, 0.3429, 0.0000, 0.0964, 0.1492, 0.3974, 0.3817, 0.3084, 0.1846,\n","        0.2887], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 2300 --------------------\n","loss tensor(11.7058, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[-0.31924642 -0.43399368 -0.11818579]\n"," [ 0.28284431  0.25819048  0.06203221]\n"," [ 0.52176675 -0.40216809 -0.23140877]\n"," [-0.18900791  0.3469643  -0.59514573]\n"," [-0.1584536   0.03887473  0.66993   ]]\n","true force: [[-0.40945609 -0.29384711 -0.02858636]\n"," [ 0.12352472  0.15232703  0.07429463]\n"," [ 0.67758992 -0.48108505 -0.32363604]\n"," [-0.24839295  0.4199088  -0.64127881]\n"," [-0.1432656   0.20269633  0.91920657]]\n","energy_diff: tensor([2.6297, 0.0851, 1.8930, 2.1962, 0.0000, 1.9075, 2.1934, 2.3529, 1.1448,\n","        2.4021], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 2400 --------------------\n","loss tensor(4.5688, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[-0.18087791  0.10904287 -0.52603948]\n"," [ 0.2202941   0.29852497 -1.01675797]\n"," [ 0.38550671  0.15056709  0.12868409]\n"," [-0.92364914 -0.28417004  0.26325213]\n"," [ 0.88253408 -0.21889589  1.04762021]]\n","true force: [[-0.25881163  0.06644327 -0.47347468]\n"," [-0.28040796  0.43311587 -1.94423512]\n"," [ 0.70244987  0.07002202  1.10317976]\n"," [-0.8419529  -0.27488411  0.25343044]\n"," [ 0.67872263 -0.29469705  1.0610996 ]]\n","energy_diff: tensor([0.5663, 0.0000, 0.9353, 0.2400, 1.0847, 1.2790, 0.2086, 0.1723, 0.9707,\n","        1.0556], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 2500 --------------------\n","loss tensor(3.8633, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 0.84434206  0.31550126  0.08181901]\n"," [-0.08675588 -0.69386523 -0.16633186]\n"," [ 0.54916967  1.80330365  0.4663094 ]\n"," [-1.58585679 -0.77908492 -0.07696014]\n"," [ 0.46717716 -0.84509752 -0.3260493 ]]\n","true force: [[ 0.9544303   0.3823057   0.11557533]\n"," [-0.19886544 -0.73469075 -0.09330953]\n"," [ 0.52340291  1.72131916  0.40254354]\n"," [-1.76430834 -0.52777655 -0.08575594]\n"," [ 0.48534057 -0.84115756 -0.33905339]]\n","energy_diff: tensor([0.0842, 0.1291, 0.4480, 0.0890, 0.5106, 0.6285, 0.0125, 0.3294, 0.0000,\n","        0.2656], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 2600 --------------------\n","loss tensor(3.4837, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 0.54342372  0.12055633 -0.18178353]\n"," [ 0.01866191 -0.18195923 -0.22762101]\n"," [ 0.10602436 -0.18030792 -0.12135355]\n"," [-0.11041797 -0.22154667 -0.02849078]\n"," [ 0.26565126 -0.34124993 -0.52343833]]\n","true force: [[ 0.29798585  0.1913749  -0.02294539]\n"," [-0.06990783  0.21538306  0.15200595]\n"," [ 0.06241045 -0.12596388  0.00311535]\n"," [-0.4850917  -0.266062    0.32140047]\n"," [ 0.19460323 -0.01473208 -0.45357638]]\n","energy_diff: tensor([1.0018, 0.6600, 0.8674, 0.8881, 0.0000, 0.9681, 0.9027, 0.5400, 0.9083,\n","        1.1273], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 2700 --------------------\n","loss tensor(4.8792, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[-0.73775301 -0.29590244 -0.00299701]\n"," [-0.16580144  0.83508674  0.53766177]\n"," [ 1.61603583  0.64578811 -1.17345578]\n"," [ 0.25130752 -0.3994471   0.57151784]\n"," [-1.12979125 -0.83195048  0.03819667]]\n","true force: [[-0.8633659  -0.29785496 -0.07997131]\n"," [-0.15673962  0.76647484  0.50025014]\n"," [ 1.93362955  0.4093605  -0.93760669]\n"," [ 0.54171211 -0.38157464  0.66780259]\n"," [-1.45523613 -0.49640573 -0.15047473]]\n","energy_diff: tensor([1.0493, 0.0000, 1.1488, 1.2522, 0.8028, 0.5662, 1.1764, 0.5360, 1.2912,\n","        0.7767], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 2800 --------------------\n","loss tensor(7.9686, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 2.25915076 -0.38258134  1.63520039]\n"," [ 7.46972977 -7.19341044 -2.19313289]\n"," [ 2.18119744 -0.74868932  1.31209803]\n"," [ 0.76511503 -1.75133651  0.53937655]\n"," [ 2.19024949 -0.18812694  1.22483661]]\n","true force: [[-1.41457666  2.70516077  2.20024094]\n"," [ 0.95470712 -0.45190188 -0.07884896]\n"," [-4.47225799 -4.99896149 -0.15872989]\n"," [ 0.41701034 -0.82978485  0.91832388]\n"," [ 4.51511719  3.57548745 -2.88098597]]\n","energy_diff: tensor([0.0000, 1.3185, 1.6811, 1.2755, 1.7218, 0.6109, 1.9167, 2.3795, 1.7944,\n","        1.7842], grad_fn=<SubBackward0>) \n","\n","-------------------- Epoch 2900 --------------------\n","loss tensor(13.0885, dtype=torch.float64, grad_fn=<MulBackward0>)\n","pred_force: [[ 0.87927899  0.04798218 -0.3945849 ]\n"," [-0.58986266  0.2468155  -0.28379772]\n"," [-0.20215987 -0.67292157 -0.10560119]\n"," [-0.15620211  0.15028053  0.26629713]\n"," [ 0.21814751  0.01950833  0.38949687]]\n","true force: [[ 0.79461078  0.12847785 -0.35979182]\n"," [-0.63244026  0.29207073 -0.3842829 ]\n"," [-0.21297496 -0.72495187 -0.19205449]\n"," [-0.19773285  0.24202754  0.40625899]\n"," [ 0.24853729  0.06237575  0.52987021]]\n","energy_diff: tensor([5.6985, 5.6074, 6.0293, 5.3718, 5.5089, 0.0000, 6.1810, 5.3125, 5.3396,\n","        5.7788], grad_fn=<SubBackward0>) \n","\n","pred_energy: tensor([-2.0623, -1.8624, -2.7960, -2.8015, -2.0670, -1.8833, -1.0259, -2.9260,\n","        -2.3238, -1.3476], grad_fn=<SqueezeBackward0>)\n","true energy: tensor([5.6300, 5.8326, 4.9450, 5.1011, 5.6850, 5.9047, 7.3017, 4.7647, 5.2904,\n","        6.7514])\n"]}],"source":["learning_rate = 1e-6\n","parameters = list(model.parameters())\n","optimizer = AdamW(parameters, lr=learning_rate)\n","loss_fn = nn.L1Loss()\n","\n","epochs = 3000\n","for epoch in range(epochs):\n","    epoch_loss = 0\n","    positions, charges = generate_electrons(n_electrons, n_groups)\n","    forces, potential_energy = calculate_forces_and_potential(positions, charges)\n","    potential_energy = torch.from_numpy(potential_energy).to(device=device, dtype=torch.float32)\n","    device = 'cpu'\n","    positions = torch.from_numpy(positions).to(device=device)\n","    padding_tensor = torch.zeros((n_groups, 1, 3), device=device)\n","    positions_tensor = torch.cat([padding_tensor, positions], dim=1)\n","    positions_tensor.requires_grad = True\n","    pred_forces, pred_energy = model(positions_tensor)\n","    pred_energy = pred_energy.squeeze()\n","\n","    # for i in range(n_groups):\n","    ### use energy to train ###\n","    # loss = loss_fn(pred_energy, potential_energy).to(device=device)\n","    ### use force to train ###\n","    loss = loss_fn(pred_forces[:, 1:], torch.from_numpy(forces)).to(device=device) * 3 * n_electrons\n","        # loss = loss_fn(pred_forces[i][1:], torch.from_numpy(forces[i])).to(device=device)\n","        # epoch_loss += loss\n","    optimizer.zero_grad()\n","    loss.backward(retain_graph=True)\n","    optimizer.step()\n","    \n","    if epoch % 100 == 0:\n","        print(\"-\"*20, \"Epoch\", epoch, \"-\"*20)\n","        print(\"loss\", loss)\n","        print(\"pred_force:\", pred_forces[0,1:].detach().numpy())\n","        print(\"true force:\", forces[0])\n","        print(\"energy_diff:\", pred_energy-potential_energy-min(pred_energy-potential_energy), \"\\n\")\n","        # print(\"true energy:\", potential_energy, \"\\n\")\n","print(\"pred_energy:\", pred_energy)\n","print(\"true energy:\", potential_energy)\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([7.6923, 7.6951, 7.7410, 7.9025, 7.7520, 7.7880, 8.3276, 7.6907, 7.6142,\n","        8.0990], grad_fn=<SubBackward0>)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["potential_energy - pred_energy"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["(array([[ 0.14452395,  0.3603397 ,  0.08955894],\n","        [-0.09670379,  0.42073898, -0.99655067],\n","        [-0.68156128,  0.32000283,  0.55275471],\n","        [ 0.10994643, -0.68132027, -0.66032615],\n","        [ 0.45297355, -0.26133985,  0.47834905]]),\n"," array([[ 0.92717188,  0.27600466,  0.80216212],\n","        [-0.75420111,  0.59891876, -1.3950508 ],\n","        [-0.58388106,  0.07083495,  0.47482792],\n","        [-0.05501321, -0.75061304, -0.62454082],\n","        [ 0.46592351, -0.19514533,  0.74260158]]))"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["pred_forces[0, 1:].detach().numpy(), forces[0]"]}],"metadata":{"kernelspec":{"display_name":"huggingface","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":2}
