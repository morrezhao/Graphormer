{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"metadata":{}},"outputs":[],"source":["import numpy as np\n","\n","def generate_electrons(n_electrons, n_groups):\n","    # 生成每个群的随机三维坐标以及电荷，目前默认电子所以电荷数都是1.\n","    # 使用下面那一行注释内容就可以把电荷数扩充到1-10之间的随机数\n","    positions = np.random.randn(n_groups, n_electrons, 3)\n","    # charges = -np.ones((n_groups, n_electrons), dtype=int)  # 每个电子带负电\n","    charges = -np.random.randint(1, 11, size=(n_groups, n_electrons))  # 每个原子带电1-10\n","    return positions, charges\n","\n","\n","def calculate_forces_and_potential(positions, charges):\n","    k_e = 1  # 库伦常数，用了原子单位制\n","    n_groups, n_electrons, _ = positions.shape\n","    forces = np.zeros_like(positions)\n","    potential_energy = np.zeros(n_groups, dtype=np.float32)\n","\n","    for group in range(n_groups):\n","        for i in range(n_electrons):\n","            for j in range(i + 1, n_electrons):\n","                r_vec = positions[group, j] - positions[group, i]\n","                r_mag = np.linalg.norm(r_vec)\n","                if r_mag == 0:\n","                    continue  # 避免两个随机出来的坐标恰好完全相同\n","                r_hat = r_vec / r_mag\n","\n","                force_magnitude = k_e * charges[group, i] * charges[group, j] / r_mag ** 2\n","                forces[group, i] -= force_magnitude * r_hat\n","                forces[group, j] += force_magnitude * r_hat\n","                potential_energy[group] += k_e * charges[group, i] * charges[group, j] / r_mag\n","\n","    return forces, potential_energy\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"metadata":{}},"outputs":[],"source":["import torch\n","\n","n_electrons = 5  # 每个电子群中的电子数量\n","n_groups = 10  # 生成10个电子群\n","\n","# device = 'cpu'\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","positions, charges = generate_electrons(n_electrons, n_groups)\n","forces, potential_energy = calculate_forces_and_potential(positions, charges)\n","potential_energy = torch.from_numpy(potential_energy)\n","positions_tensor = torch.from_numpy(positions).to(device=device)\n","padding_tensor = torch.zeros((n_groups, 1, 3), device=device)\n","positions_tensor = torch.cat([padding_tensor, positions_tensor], dim=1)\n","positions_tensor.requires_grad = True\n","\n","# 这里用来生成和处理数据"]},{"cell_type":"code","execution_count":3,"metadata":{"metadata":{}},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/mamba/envs/hf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Some weights of GraphormerModel were not initialized from the model checkpoint at clefourrier/graphormer-base-pcqm4mv1 and are newly initialized: ['graph_encoder.emb_layer_norm.bias', 'graph_encoder.emb_layer_norm.weight', 'graph_encoder.graph_attn_bias.edge_dis_encoder.weight', 'graph_encoder.graph_attn_bias.edge_encoder.weight', 'graph_encoder.graph_attn_bias.graph_token_virtual_distance.weight', 'graph_encoder.graph_attn_bias.spatial_pos_encoder.weight', 'graph_encoder.graph_node_feature.atom_encoder.weight', 'graph_encoder.graph_node_feature.graph_token.weight', 'graph_encoder.graph_node_feature.in_degree_encoder.weight', 'graph_encoder.graph_node_feature.out_degree_encoder.weight', 'graph_encoder.layers.0.fc1.bias', 'graph_encoder.layers.0.fc1.weight', 'graph_encoder.layers.0.fc2.bias', 'graph_encoder.layers.0.fc2.weight', 'graph_encoder.layers.0.final_layer_norm.bias', 'graph_encoder.layers.0.final_layer_norm.weight', 'graph_encoder.layers.0.self_attn.k_proj.bias', 'graph_encoder.layers.0.self_attn.k_proj.weight', 'graph_encoder.layers.0.self_attn.out_proj.bias', 'graph_encoder.layers.0.self_attn.out_proj.weight', 'graph_encoder.layers.0.self_attn.q_proj.bias', 'graph_encoder.layers.0.self_attn.q_proj.weight', 'graph_encoder.layers.0.self_attn.v_proj.bias', 'graph_encoder.layers.0.self_attn.v_proj.weight', 'graph_encoder.layers.0.self_attn_layer_norm.bias', 'graph_encoder.layers.0.self_attn_layer_norm.weight', 'graph_encoder.layers.1.fc1.bias', 'graph_encoder.layers.1.fc1.weight', 'graph_encoder.layers.1.fc2.bias', 'graph_encoder.layers.1.fc2.weight', 'graph_encoder.layers.1.final_layer_norm.bias', 'graph_encoder.layers.1.final_layer_norm.weight', 'graph_encoder.layers.1.self_attn.k_proj.bias', 'graph_encoder.layers.1.self_attn.k_proj.weight', 'graph_encoder.layers.1.self_attn.out_proj.bias', 'graph_encoder.layers.1.self_attn.out_proj.weight', 'graph_encoder.layers.1.self_attn.q_proj.bias', 'graph_encoder.layers.1.self_attn.q_proj.weight', 'graph_encoder.layers.1.self_attn.v_proj.bias', 'graph_encoder.layers.1.self_attn.v_proj.weight', 'graph_encoder.layers.1.self_attn_layer_norm.bias', 'graph_encoder.layers.1.self_attn_layer_norm.weight', 'graph_encoder.layers.10.fc1.bias', 'graph_encoder.layers.10.fc1.weight', 'graph_encoder.layers.10.fc2.bias', 'graph_encoder.layers.10.fc2.weight', 'graph_encoder.layers.10.final_layer_norm.bias', 'graph_encoder.layers.10.final_layer_norm.weight', 'graph_encoder.layers.10.self_attn.k_proj.bias', 'graph_encoder.layers.10.self_attn.k_proj.weight', 'graph_encoder.layers.10.self_attn.out_proj.bias', 'graph_encoder.layers.10.self_attn.out_proj.weight', 'graph_encoder.layers.10.self_attn.q_proj.bias', 'graph_encoder.layers.10.self_attn.q_proj.weight', 'graph_encoder.layers.10.self_attn.v_proj.bias', 'graph_encoder.layers.10.self_attn.v_proj.weight', 'graph_encoder.layers.10.self_attn_layer_norm.bias', 'graph_encoder.layers.10.self_attn_layer_norm.weight', 'graph_encoder.layers.11.fc1.bias', 'graph_encoder.layers.11.fc1.weight', 'graph_encoder.layers.11.fc2.bias', 'graph_encoder.layers.11.fc2.weight', 'graph_encoder.layers.11.final_layer_norm.bias', 'graph_encoder.layers.11.final_layer_norm.weight', 'graph_encoder.layers.11.self_attn.k_proj.bias', 'graph_encoder.layers.11.self_attn.k_proj.weight', 'graph_encoder.layers.11.self_attn.out_proj.bias', 'graph_encoder.layers.11.self_attn.out_proj.weight', 'graph_encoder.layers.11.self_attn.q_proj.bias', 'graph_encoder.layers.11.self_attn.q_proj.weight', 'graph_encoder.layers.11.self_attn.v_proj.bias', 'graph_encoder.layers.11.self_attn.v_proj.weight', 'graph_encoder.layers.11.self_attn_layer_norm.bias', 'graph_encoder.layers.11.self_attn_layer_norm.weight', 'graph_encoder.layers.2.fc1.bias', 'graph_encoder.layers.2.fc1.weight', 'graph_encoder.layers.2.fc2.bias', 'graph_encoder.layers.2.fc2.weight', 'graph_encoder.layers.2.final_layer_norm.bias', 'graph_encoder.layers.2.final_layer_norm.weight', 'graph_encoder.layers.2.self_attn.k_proj.bias', 'graph_encoder.layers.2.self_attn.k_proj.weight', 'graph_encoder.layers.2.self_attn.out_proj.bias', 'graph_encoder.layers.2.self_attn.out_proj.weight', 'graph_encoder.layers.2.self_attn.q_proj.bias', 'graph_encoder.layers.2.self_attn.q_proj.weight', 'graph_encoder.layers.2.self_attn.v_proj.bias', 'graph_encoder.layers.2.self_attn.v_proj.weight', 'graph_encoder.layers.2.self_attn_layer_norm.bias', 'graph_encoder.layers.2.self_attn_layer_norm.weight', 'graph_encoder.layers.3.fc1.bias', 'graph_encoder.layers.3.fc1.weight', 'graph_encoder.layers.3.fc2.bias', 'graph_encoder.layers.3.fc2.weight', 'graph_encoder.layers.3.final_layer_norm.bias', 'graph_encoder.layers.3.final_layer_norm.weight', 'graph_encoder.layers.3.self_attn.k_proj.bias', 'graph_encoder.layers.3.self_attn.k_proj.weight', 'graph_encoder.layers.3.self_attn.out_proj.bias', 'graph_encoder.layers.3.self_attn.out_proj.weight', 'graph_encoder.layers.3.self_attn.q_proj.bias', 'graph_encoder.layers.3.self_attn.q_proj.weight', 'graph_encoder.layers.3.self_attn.v_proj.bias', 'graph_encoder.layers.3.self_attn.v_proj.weight', 'graph_encoder.layers.3.self_attn_layer_norm.bias', 'graph_encoder.layers.3.self_attn_layer_norm.weight', 'graph_encoder.layers.4.fc1.bias', 'graph_encoder.layers.4.fc1.weight', 'graph_encoder.layers.4.fc2.bias', 'graph_encoder.layers.4.fc2.weight', 'graph_encoder.layers.4.final_layer_norm.bias', 'graph_encoder.layers.4.final_layer_norm.weight', 'graph_encoder.layers.4.self_attn.k_proj.bias', 'graph_encoder.layers.4.self_attn.k_proj.weight', 'graph_encoder.layers.4.self_attn.out_proj.bias', 'graph_encoder.layers.4.self_attn.out_proj.weight', 'graph_encoder.layers.4.self_attn.q_proj.bias', 'graph_encoder.layers.4.self_attn.q_proj.weight', 'graph_encoder.layers.4.self_attn.v_proj.bias', 'graph_encoder.layers.4.self_attn.v_proj.weight', 'graph_encoder.layers.4.self_attn_layer_norm.bias', 'graph_encoder.layers.4.self_attn_layer_norm.weight', 'graph_encoder.layers.5.fc1.bias', 'graph_encoder.layers.5.fc1.weight', 'graph_encoder.layers.5.fc2.bias', 'graph_encoder.layers.5.fc2.weight', 'graph_encoder.layers.5.final_layer_norm.bias', 'graph_encoder.layers.5.final_layer_norm.weight', 'graph_encoder.layers.5.self_attn.k_proj.bias', 'graph_encoder.layers.5.self_attn.k_proj.weight', 'graph_encoder.layers.5.self_attn.out_proj.bias', 'graph_encoder.layers.5.self_attn.out_proj.weight', 'graph_encoder.layers.5.self_attn.q_proj.bias', 'graph_encoder.layers.5.self_attn.q_proj.weight', 'graph_encoder.layers.5.self_attn.v_proj.bias', 'graph_encoder.layers.5.self_attn.v_proj.weight', 'graph_encoder.layers.5.self_attn_layer_norm.bias', 'graph_encoder.layers.5.self_attn_layer_norm.weight', 'graph_encoder.layers.6.fc1.bias', 'graph_encoder.layers.6.fc1.weight', 'graph_encoder.layers.6.fc2.bias', 'graph_encoder.layers.6.fc2.weight', 'graph_encoder.layers.6.final_layer_norm.bias', 'graph_encoder.layers.6.final_layer_norm.weight', 'graph_encoder.layers.6.self_attn.k_proj.bias', 'graph_encoder.layers.6.self_attn.k_proj.weight', 'graph_encoder.layers.6.self_attn.out_proj.bias', 'graph_encoder.layers.6.self_attn.out_proj.weight', 'graph_encoder.layers.6.self_attn.q_proj.bias', 'graph_encoder.layers.6.self_attn.q_proj.weight', 'graph_encoder.layers.6.self_attn.v_proj.bias', 'graph_encoder.layers.6.self_attn.v_proj.weight', 'graph_encoder.layers.6.self_attn_layer_norm.bias', 'graph_encoder.layers.6.self_attn_layer_norm.weight', 'graph_encoder.layers.7.fc1.bias', 'graph_encoder.layers.7.fc1.weight', 'graph_encoder.layers.7.fc2.bias', 'graph_encoder.layers.7.fc2.weight', 'graph_encoder.layers.7.final_layer_norm.bias', 'graph_encoder.layers.7.final_layer_norm.weight', 'graph_encoder.layers.7.self_attn.k_proj.bias', 'graph_encoder.layers.7.self_attn.k_proj.weight', 'graph_encoder.layers.7.self_attn.out_proj.bias', 'graph_encoder.layers.7.self_attn.out_proj.weight', 'graph_encoder.layers.7.self_attn.q_proj.bias', 'graph_encoder.layers.7.self_attn.q_proj.weight', 'graph_encoder.layers.7.self_attn.v_proj.bias', 'graph_encoder.layers.7.self_attn.v_proj.weight', 'graph_encoder.layers.7.self_attn_layer_norm.bias', 'graph_encoder.layers.7.self_attn_layer_norm.weight', 'graph_encoder.layers.8.fc1.bias', 'graph_encoder.layers.8.fc1.weight', 'graph_encoder.layers.8.fc2.bias', 'graph_encoder.layers.8.fc2.weight', 'graph_encoder.layers.8.final_layer_norm.bias', 'graph_encoder.layers.8.final_layer_norm.weight', 'graph_encoder.layers.8.self_attn.k_proj.bias', 'graph_encoder.layers.8.self_attn.k_proj.weight', 'graph_encoder.layers.8.self_attn.out_proj.bias', 'graph_encoder.layers.8.self_attn.out_proj.weight', 'graph_encoder.layers.8.self_attn.q_proj.bias', 'graph_encoder.layers.8.self_attn.q_proj.weight', 'graph_encoder.layers.8.self_attn.v_proj.bias', 'graph_encoder.layers.8.self_attn.v_proj.weight', 'graph_encoder.layers.8.self_attn_layer_norm.bias', 'graph_encoder.layers.8.self_attn_layer_norm.weight', 'graph_encoder.layers.9.fc1.bias', 'graph_encoder.layers.9.fc1.weight', 'graph_encoder.layers.9.fc2.bias', 'graph_encoder.layers.9.fc2.weight', 'graph_encoder.layers.9.final_layer_norm.bias', 'graph_encoder.layers.9.final_layer_norm.weight', 'graph_encoder.layers.9.self_attn.k_proj.bias', 'graph_encoder.layers.9.self_attn.k_proj.weight', 'graph_encoder.layers.9.self_attn.out_proj.bias', 'graph_encoder.layers.9.self_attn.out_proj.weight', 'graph_encoder.layers.9.self_attn.q_proj.bias', 'graph_encoder.layers.9.self_attn.q_proj.weight', 'graph_encoder.layers.9.self_attn.v_proj.bias', 'graph_encoder.layers.9.self_attn.v_proj.weight', 'graph_encoder.layers.9.self_attn_layer_norm.bias', 'graph_encoder.layers.9.self_attn_layer_norm.weight', 'layer_norm.bias', 'layer_norm.weight', 'lm_head_transform_weight.bias', 'lm_head_transform_weight.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# 使用预定义的模型，为了简化，multi_hop_max_dist取0\n","# 如果使用的不是dp的服务器，请注释掉os.environ这两行\n","from transformers import GraphormerModel, AdamW, GraphormerConfig\n","import os\n","\n","###\n","os.environ['HTTP_PROXY'] = 'http://ga.dp.tech:8118'\n","os.environ['HTTPS_PROXY'] = 'http://ga.dp.tech:8118'\n","###\n","\n","model_name = \"clefourrier/graphormer-base-pcqm4mv1\"\n","\n","backbone_model = GraphormerModel.from_pretrained(model_name)\n","config = GraphormerConfig.from_pretrained(model_name)\n","config.multi_hop_max_dist = 0"]},{"cell_type":"code","execution_count":4,"metadata":{"metadata":{}},"outputs":[],"source":["from transformers import GraphormerModel\n","import torch\n","import torch.nn as nn\n","\n","# 定义新的注意力头，用于预测势能\n","\n","class CustomGraphormerModel(nn.Module):\n","    def __init__(self, graphormer_model):\n","        super().__init__()\n","        self.graphormer = graphormer_model\n","        # self.force_head = nn.Linear(768, 3)  # 输出力的线性层，假设每个原子有三个力分量（不需要，力通过势能求导直接得到）\n","        self.energy_head = nn.Linear(768, 1)  # 输出势能的线性层\n","    \n","    def convert_to_inputs(self, positions):\n","        n_groups, n_electrons, _ = positions.shape\n","        # 这是因为graphormer引入了一个虚节点，position[:, 0]都是占位符[0, 0, 0]，是没有任何意义的，所以force[:,1:]才有意义。真实的电子数是positions.shape[1]-1\n","        n_electrons -= 1\n","        device = 'cuda' if torch.cuda.is_available() else 'cpu' \n","        # （服务器内存好像不太够，先用cpu了）\n","        # device = 'cpu'\n","\n","        # 初始化Tensor\n","        # input_nodes = torch.ones((n_groups, n_electrons, 1), device=device, dtype=int)  # [num_graphs, num_nodes, 1]\n","        input_nodes = -torch.from_numpy(charges).unsqueeze(-1).to(device=device)\n","        input_edges = torch.zeros((n_groups, n_electrons, n_electrons, 1, 1), device=device, dtype=int) # [num_graphs, num_nodes, num_nodes, max_hop, num_edge_features]\n","        # attn_bias = torch.zeros((n_groups, n_electrons + 1, n_electrons + 1), device=device)  \n","        in_degree = torch.ones((n_groups, n_electrons), device=device, dtype=int) * (n_electrons - 1)\n","        out_degree = torch.ones((n_groups, n_electrons), device=device, dtype=int) * (n_electrons - 1)\n","        spatial_pos = torch.zeros((n_groups, n_electrons, n_electrons), device=device, dtype=int)  \n","        # spatial_pos = torch.arange(n_electrons).unsqueeze(0).repeat(n_groups, 1).to(device)\n","        attn_edge_type = torch.zeros((n_groups, n_electrons, n_electrons), device=device, dtype=int)\n","\n","        # 计算边特征为距离的倒数\n","        for group in range(n_groups):\n","            for i in range(n_electrons):\n","                for j in range(n_electrons):\n","                    if i != j:\n","                        # distance = torch.norm(positions[group, i] - positions[group, j])\n","                        # attn_bias[group, i + 1, j + 1] = distance\n","                        input_edges[group, i, j, 0, 0] = 1\n","                        spatial_pos[group, i, j] = 1\n","        expanded_positions_i = positions.unsqueeze(2).repeat(1, 1, n_electrons + 1, 1)\n","        expanded_positions_j = positions.unsqueeze(1).repeat(1, n_electrons + 1, 1, 1)\n","        # print(expanded_positions_i.shape, expanded_positions_j.shape)\n","\n","    # 计算所有配对的欧几里得距离\n","        attn_bias = torch.norm(expanded_positions_i - expanded_positions_j, dim=3)\n","        # print(\"positions_grad:\", torch.autograd.grad(attn_bias[0][1][2], positions, create_graph=True)[0])\n","\n","        graphormer_inputs = {\n","            'input_nodes': input_nodes, # 电荷信息encode进了这里\n","            'input_edges': input_edges,\n","            'attn_bias': attn_bias, # 相对距离信息encode进了这里\n","            'in_degree': in_degree,\n","            'out_degree': out_degree,\n","            'spatial_pos': spatial_pos, \n","            'attn_edge_type': attn_edge_type\n","        }\n","        \n","        return graphormer_inputs\n","\n","    def forward(self, positions):\n","        inputs = self.convert_to_inputs(positions=positions)\n","        outputs = self.graphormer(**inputs)\n","        pooled_output = outputs.last_hidden_state.mean(dim=1)  # 从Graphormer输出中提取池化表示\n","        # forces = self.force_head(pooled_output)  # 预测力  \n","        energy = self.energy_head(pooled_output)  # 预测势能\n","        # forces_1 = torch.zeros_like(positions)\n","        # print(torch.autograd.grad(energy[0], self.positions, create_graph=True, allow_unused=True))\n","        grad_outputs = torch.ones_like(energy)\n","        forces = -torch.autograd.grad(energy, positions, create_graph=True, retain_graph=True, grad_outputs=grad_outputs)[0]     \n","        # 用两种方法算了力，可以确定结果是对的。     \n","        # for i in range(energy.shape[0]): \n","        #     # print(torch.autograd.grad(energy[i], self.positions, create_graph=True)[0][i])\n","        #     forces_1[i] = -torch.autograd.grad(energy[i], positions, retain_graph=True)[0][i] \n","        # assert torch.allclose(forces, forces_1)\n","        return forces, energy\n","    \n","\n","\n","# positions_tensor = torch.from_numpy(positions).to(device=device)\n","# padding_tensor = torch.zeros((n_groups, 1, 3), device=device)\n","# positions_tensor = torch.cat([padding_tensor, positions_tensor], dim=1)\n","# positions_tensor.requires_grad = True\n","model = CustomGraphormerModel(backbone_model).to(device=device)"]},{"cell_type":"code","execution_count":24,"metadata":{"metadata":{}},"outputs":[],"source":["### use for debug\n","\n","# positions_tensor.requires_grad = True\n","\n","# expanded_positions_i = positions_tensor.unsqueeze(2).repeat(1, 1, n_electrons + 1, 1)\n","# expanded_positions_j = positions_tensor.unsqueeze(1).repeat(1, n_electrons + 1, 1, 1)\n","# attn_bias = torch.norm(expanded_positions_i - expanded_positions_j, dim=3)\n","# # attn_bias[0][1][2]\n","# print(\"positions_grad:\", torch.autograd.grad(attn_bias[0][1][2], positions_tensor, create_graph=True)[0])\n","# pred_forces, pred_energy = model(positions_tensor)"]},{"cell_type":"code","execution_count":5,"metadata":{"metadata":{}},"outputs":[],"source":["# pred_energy\n","# potential_energy = torch.from_numpy(potential_energy)\n","# loss = loss_fn(pred_energy[0], potential_energy[0])\n","# loss.backward"]},{"cell_type":"code","execution_count":10,"metadata":{"metadata":{}},"outputs":[{"name":"stdout","output_type":"stream","text":["pred_energy: tensor([ -86.4364,  -95.4460, -114.1423,  -97.3549,  -52.9868, -112.6838,\n","        -135.9707,  -81.2149, -110.2545, -135.7096], device='cuda:0',\n","       grad_fn=<SqueezeBackward0>)\n","true energy: tensor([126.3995,  57.0571,  63.5348, 127.5742, 263.2089, 233.5976, 166.9610,\n","        151.9480,  78.3915,  85.3786], device='cuda:0')\n"]}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu' \n","learning_rate = 1e-5\n","parameters = list(model.parameters())\n","optimizer = AdamW(parameters, lr=learning_rate)\n","\n","# 用L1Loss作为损失函数，也可以换成MSE试一试\n","loss_fn = nn.L1Loss()\n","\n","# 要训练更多轮才能够收敛\n","epochs = 30000\n","with open(\"training_loss.txt\", \"w\") as f:\n","    for epoch in range(epochs):\n","        epoch_loss = 0\n","        positions, charges = generate_electrons(n_electrons, n_groups)\n","        forces, potential_energy = calculate_forces_and_potential(positions, charges)\n","        potential_energy = torch.from_numpy(potential_energy).to(device=device, dtype=torch.float32)\n","        \n","        positions = torch.from_numpy(positions).to(device=device)\n","        padding_tensor = torch.zeros((n_groups, 1, 3), device=device)\n","        positions_tensor = torch.cat([padding_tensor, positions], dim=1).to(device=device)\n","        positions_tensor.requires_grad = True\n","        pred_forces, pred_energy = model(positions_tensor)\n","        pred_energy = pred_energy.squeeze()\n","\n","        ### use energy to train ###\n","        # loss = loss_fn(pred_energy, potential_energy).to(device=device)\n","        ### use force to train ###\n","        \n","        loss = loss_fn(pred_forces[:, 1:], torch.from_numpy(forces).to(device=device))\n","        optimizer.zero_grad()\n","        loss.backward(retain_graph=True)\n","        optimizer.step()\n","        if epoch % 100 == 0:\n","            f.write(\"-\"*20 + f\" Epoch {epoch} \" + \"-\"*20 + \"\\n\")\n","            f.write(f\"loss: {loss}\\n\")\n","            f.write(f\"pred_force: {pred_forces[0,1:].cpu().detach().numpy()}\\n\")\n","            f.write(f\"true force: {forces[0]}\\n\")\n","            f.write(f\"energy_diff: {pred_energy-potential_energy-min(pred_energy-potential_energy)}\\n\\n\")\n","            # 当用导数作为损失的时候，原函数可能会相差一个常数\n","        # print(\"true energy:\", potential_energy, \"\\n\")\n","print(\"pred_energy:\", pred_energy)\n","print(\"true energy:\", potential_energy)\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([212.8359, 152.5031, 177.6771, 224.9291, 316.1957, 346.2814, 302.9317,\n","        233.1628, 188.6459, 221.0882], device='cuda:0', grad_fn=<SubBackward0>)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["potential_energy - pred_energy"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["(array([[ 29.01672432,  29.606307  , -11.73352724],\n","        [ 10.75190692,  -8.78998926,  18.41565162],\n","        [-11.34183871,  -3.91693026,   5.70623268],\n","        [-29.08767887, -18.38089286, -23.12533671],\n","        [  0.66608919,   0.82603205,   9.72372213]]),\n"," array([[ 28.51726954,  29.13318947, -12.36860495],\n","        [ 15.67465045,  -7.70215241,  21.59097024],\n","        [-12.20445575,  -4.39830857,   4.86621647],\n","        [-35.18274973, -19.64854352, -25.7651013 ],\n","        [  3.1952855 ,   2.61581503,  11.67651953]]))"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["pred_forces[0, 1:].detach().cpu().numpy(), forces[0]"]}],"metadata":{"kernelspec":{"display_name":"huggingface","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":2}
